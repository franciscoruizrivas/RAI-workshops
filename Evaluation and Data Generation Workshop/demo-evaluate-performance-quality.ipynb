{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Using Performance & Quality Metrics\n",
    "\n",
    "Contoso Zoo is developing an AI-powered smart guide app to enhance visitors' experiences. The app aims to answer questions about wildlife. In this exercise, you will evaluate the model output for conversation pairs which reflect a conversation between a Contoso Zoo visitor and the smart guide AI assistant. Provided below is an example of a row of data provided for a question and answer pair for the Contoso Zoo app:\n",
    "\n",
    "`query`\n",
    "\n",
    "Why do flamingos stand on one leg?\n",
    "\n",
    "`context`\n",
    "\n",
    "Flamingos stand on one leg as a thermoregulatory behavior to minimize heat loss while wading in water. This posture also reduces muscle fatigue by engaging a passive locking mechanism in their joints, allowing them to rest efficiently.\n",
    "\n",
    "`ground_truth`\n",
    "\n",
    "Flamingos stand on one leg to conserve body heat and reduce muscle fatigue. Their joints lock in place, allowing them to balance effortlessly while resting. This helps them stay warm, especially when standing in cold water.\n",
    "\n",
    "`response`\n",
    "\n",
    "Fantastic observation! Flamingos stand on one leg to conserve body heat. By tucking one leg close to their body, they lose less heat through their limbs. Plus, their joints lock into place, so it's actually comfortable for them—like their version of lounging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add environment variables to the .env file\n",
    "\n",
    "In the root of the **Evaluation and Data Generation Workshop** folder is an `.env` file. Within the `.env` file, fill in the values for the environment variables. You can locate the values for each environment variable in the following locations of the [Azure AI Foundry](https://ai.azure.com) portal:\n",
    "\n",
    "- `AZURE_SUBSCRIPTION_ID` - On the **Overview** page of your project within **Project details**.\n",
    "- `AZURE_AI_PROJECT_NAME` - At the top of the **Overview** page for your project.\n",
    "- `AZURE_OPENAI_RESOURCE_GROUP` - On the **Overview** page of the **Management Center** within **Project properties**.\n",
    "- `AZURE_OPENAI_SERVICE` - On the **Overview** page of your project in the **Included capabilities** tab for **Azure OpenAI Service**.\n",
    "- `AZURE_OPENAI_API_VERSION` - On the [API version lifecycle](https://learn.microsoft.com/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) webpage within the **Latest GA API release** section.\n",
    "- `AZURE_OPENAI_ENDPOINT` - On the **Details** tab of your model deployment within **Endpoint** (i.e. **Target URI**)\n",
    "- `AZURE_OPENAI_DEPLOYMENT_NAME` -  On the **Details** tab of your model deployment within **Deployment info**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign in to Azure\n",
    "\n",
    "As a security best practice, we'll use [keyless authentication](https://learn.microsoft.com/azure/developer/ai/keyless-connections?tabs=csharp%2Cazure-cli) to authenticate to Azure OpenAI with Microsoft Entra ID. Before you can do so, you'll first need to install the **Azure CLI** per the [installation instructions](https://learn.microsoft.com/cli/azure/install-azure-cli) for your operating system.\n",
    "\n",
    "Next, open a terminal and run `az login` to sign in to your Azure account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package\n",
    "\n",
    "The evaluator classes for assessing performance and quality are in the Azure AI Evaluation SDK. We'll begin by installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the environment variables.\n",
    "\n",
    "We'll import `os` and `load_dotenv` so that you can access the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup keyless authentication\n",
    "\n",
    "Rather than hardcode your **key**, we'll use a keyless connection with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.identity\n",
    "\n",
    "credential = azure.identity.DefaultAzureCredential()\n",
    "token_provider = azure.identity.get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "token = token_provider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "The `model_config` is necessary as it's a required parameter when creating an instance of the evaluator class. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API key\n",
    "- Azure deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": token,\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create variables for the evaluation data\n",
    "\n",
    "Since we'll be using the same context, query, response, and ground truth for the exercises, we'll create a variable to store each string and pass the variables into our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why do flamingos stand on one leg?\"\n",
    "context = \"Flamingos stand on one leg as a thermoregulatory behavior to minimize heat loss while wading in water. This posture also reduces muscle fatigue by engaging a passive locking mechanism in their joints, allowing them to rest efficiently.\"\n",
    "ground_truth = \"Flamingos stand on one leg to conserve body heat and reduce muscle fatigue. Their joints lock in place, allowing them to balance effortlessly while resting. This helps them stay warm, especially when standing in cold water.\"\n",
    "response = \"Fantastic observation! Flamingos stand on one leg to conserve body heat. By tucking one leg close to their body, they lose less heat through their limbs. Plus, their joints lock into place, so it's actually comfortable for them—like their version of lounging!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for Groundedness\n",
    "\n",
    "Create an instance of the `GroundednessEvaluator` and run the evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "\n",
    "groundedness_score = groundedness_eval(\n",
    "    response=response,\n",
    "    context=context,\n",
    ")\n",
    "\n",
    "print(groundedness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for Relevance\n",
    "\n",
    "Create an instance of the `RelevanceEvaluator` and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "relevance_score = relevance_eval(\n",
    "    response=response,\n",
    "    context=context,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for Coherence\n",
    "\n",
    "Create an instance of the `CoherenceEvaluator` and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "\n",
    "coherence_score = coherence_eval(\n",
    "    response=response,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for Fluency\n",
    "\n",
    "Create an instance of the `FluencyEvaluator` and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "\n",
    "fluency_score = fluency_eval(\n",
    "    response=response,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(fluency_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for Similarity\n",
    "\n",
    "Create an instance of the `SimiliartyEvaluator` and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n",
    "similarity_score = similarity_eval(\n",
    "    response=response,\n",
    "    query=query,\n",
    "    ground_truth=ground_truth\n",
    ")\n",
    "\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for F1 Score\n",
    "\n",
    "Create an instance of the `F1ScoreEvaluator` and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "\n",
    "f1_score = f1_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth\n",
    ")\n",
    "\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a dataset\n",
    "\n",
    "To evaluate the quality of the responses from the app, a synthetic question-answer dataset was generated that covers various aspects of wildlife, including animal behavior, habitat preferences, and dietary needs.\n",
    "\n",
    "You'll now evaluate the model output for the conversation pairs. Provided below is an example of a row of data provided for a question and answer pair for the Contoso Zoo app:\n",
    "\n",
    "`query`\n",
    "\n",
    "\"Wow, the lion is much bigger than I expected! How much do they typically weigh?\"\n",
    "\n",
    "`response`\n",
    "\n",
    "\"Lions are indeed impressive creatures! Their weight varies depending on their sex and geographic location. Here's a general breakdown:\\n\\n- **Male lions** typically weigh between **330 to 550 pounds (150 to 250 kilograms)**.\\n- **Female lions**, being smaller, usually weigh between **260 to 400 pounds (120 to 180 kilograms)**.\\n\\nLions in certain regions, such as southern Africa, tend to be larger on average compared to those in other parts of their range. Their size and muscular build are part of what makes them such formidable predators!\"\n",
    "\n",
    "We can run an evaluation for a dataset with the `evaluate` function. In addition, we can run the evaluation using multiple evaluators. In our case, we're going to run an evaluation for the dataset within the `zoo_app_conversations.jsonl` file. We'll also output the results to a new `evaluation_results.json` file.\n",
    "\n",
    "Let's run an evaluation using the `Relevance`, `Coherence`, and `Fluency` evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import json\n",
    "\n",
    "path = \"zoo_app_conversations.jsonl\"\n",
    "\n",
    "result = evaluate(\n",
    "    data=path, # provide your data here\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results with Pretty Print\n",
    "\n",
    "Now that we've run the evaluation, let's print the results using Pretty Print, which displays data in a structured and visually appealing way, making it easier to read and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results as table\n",
    "\n",
    "We can also print the results as a table using `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "If you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.\n",
    "\n",
    "**Note**: You may be prompted to delete your deployed model(s) before deleting the resource group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
